blogdown::serve_site()
blogdown::stop_server()
install.packages(c("blogdown", "hugo"))
install.packages(c("blogdown", "hugo"))
blogdown::new_site(theme = "wowchemy/starter-academic")
y
blogdown::new_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::build_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
hugo --gc
hugo --gc
cd
getwd()
setwd("C:/Users/Nino/Documents/GitHub/ninobuliskeria.github.io/config/_default")
hugo --gc
blogdown::build_site()
setwd("C:/Users/Nino/Documents/GitHub/ninobuliskeria.github.io")
blogdown::build_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::stop_server()
blogdown::build_site()
blogdown::serve_site()
blogdown::stop_server()
blogdown::build_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
if (!require(rvest)) install.packages('rvest')
if (!require(tframePlus)) install.packages("tframePlus")
if (!require(tictoc)) install.packages("tictoc")
suppressPackageStartupMessages({
library('rvest')
library('xml2')
library('tseries')
library('stringr')
library('plyr')
library('tframePlus')
library('zoo')
library('xts')
library('lubridate')
library('tictoc')
})
suppressPackageStartupMessages({
library('rvest')
library('xml2')
library('tseries')
library('stringr')
library('plyr')
library('tframePlus')
library('zoo')
library('xts')
library('lubridate')
library('tictoc')
library('reshape2')
library('rapportools')
})
wd()
econ    = c('economie', 'economique', 'economiques')
tax     = c('taxe', 'taxes', 'impot', 'impots', 'politique', 'politiques', 'regulation', 'regulations', 'reglementation', 'loi', 'lois+reglementations', 'depense', 'depenses',  'deficit', 'deficits', 'banque+centrale', 'BCE', 'Reserve+Federale', 'budget', 'budgetaire')
uncert  = c('incertitude', 'incertain',  'incertitudes' , 'incertains')
#time periods
start   = c('01/01/1991', '01/01/1993', '01/01/1995', '01/01/1997', '01/01/1999', '01/01/2001', '01/01/2003', '01/01/2005', '01/01/2007', '01/01/2009', '01/01/2011', '01/01/2013', '01/01/2015', '01/01/2017', '01/01/2019')
end     = c('31/12/1992', '31/12/1994', '31/12/1996', '31/12/1998', '31/12/2000', '31/12/2002', '31/12/2004', '31/12/2006', '31/12/2008', '31/12/2010', '31/12/2012', '31/12/2014', '31/12/2016', '31/12/2018', '31/12/2020')
#quary1
query1 <- expand.grid(econ, tax, uncert)
STquery1 = c()
for (j in 1:length(query1$Var1)){
STquery1[j] = paste0(query1[j,1], '%2C+', query1[j,2], '%2C+', query1[j,3])
}
#get the number of pages per search keyword combination
pages <- c()
for (k in 1:length(STquery1)){
url1 = paste0('https://www.lemonde.fr/recherche/?search_keywords=', STquery1[k],'&start_at=',start[1],'&end_at=',end[15],'&search_sort=date_asc&page=',1)
webpage  <- read_html(url1)
page <- html_text(html_nodes(webpage,'.river__pagination--page-search'))
page.n <- as.numeric(page)
pages[k] <- max(page.n)
print(k)
}
View(query1)
View(webpage)
pages <- ifelse(pages == "-Inf", 1, pages)
pquery <- cbind(STquery1, pages)
View(pquery)
rm(econ, tax,uncert, j, k, page.n, page, webpage, url1, query1)
PUBLIE = list()
for (j in 1:3    ){    #loop over keywords
for (i in 1:pages[j]){          #loop over pages
url1 = paste0('https://www.lemonde.fr/recherche/?search_keywords=',STquery1[j],'&start_at=',start[1],'&end_at=',end[15],'&search_sort=date_asc&page=',i)
webpage  <- read_html(url1)
PUBLIE[[i + k[j]]] <- html_text(html_nodes(webpage,'.teaser--inline-picture'))
}
print(j)
}
k <- c()
for(j in 2:length(pages)){
k[1]=0
k[j] = k[j-1] + pages[j-1]
}
PUBLIE = list()
for (j in 1:3    ){    #loop over keywords
for (i in 1:pages[j]){          #loop over pages
url1 = paste0('https://www.lemonde.fr/recherche/?search_keywords=',STquery1[j],'&start_at=',start[1],'&end_at=',end[15],'&search_sort=date_asc&page=',i)
webpage  <- read_html(url1)
PUBLIE[[i + k[j]]] <- html_text(html_nodes(webpage,'.teaser--inline-picture'))
}
print(j)
}
View(PUBLIE)
url1
load("H:/.shortcut-targets-by-id/1-95-jjFcJ-99DqRccBGFmRS9ZqRftxBC/AnalysisOfEPU/AnalysisOfEPU_Buliskeria/DataScraping/various codes/lamonde/Q1/Q1_teaser-inline-picture.RData")
View(data2)
View(data)
View(le)
View(PUBLIE)
View(data2)
data[1]
data[1,1]
start   = c('01-01-1991', '01-01-1993', '01-01-1995', '01-01-1997', '01-01-1999', '01-01-2001', '01-01-2003', '01-01-2005', '01-01-2007', '01-01-2009', '01-01-2011', '01-01-2013', '01-01-2015', '01-01-2017', '01-01-2019')
end     = c('31-12-1992', '31-12-1994', '31-12-1996', '31-12-1998', '31-12-2000', '31-12-2002', '31-12-2004', '31-12-2006', '31-12-2008', '31-12-2010', '31-12-2012', '31-12-2014', '31-12-2016', '31-12-2018', '31-12-2020')
#quary1
econ    = c('economie', 'economique', 'economiques')
tax     = c('taxe', 'taxes', 'impot', 'impots', 'politique', 'politiques', 'regulation', 'regulations', 'reglementation', 'loi', 'lois+reglementations', 'depense', 'depenses',  'deficit', 'deficits', 'banque+centrale', 'BCE', 'Reserve+Federale', 'budget', 'budgetaire')
uncert  = c('incertitude', 'incertain',  'incertitudes' , 'incertains')
#time periods
start   = c('01-01-1991', '01-01-1993', '01-01-1995', '01-01-1997', '01-01-1999', '01-01-2001', '01-01-2003', '01-01-2005', '01-01-2007', '01-01-2009', '01-01-2011', '01-01-2013', '01-01-2015', '01-01-2017', '01-01-2019')
end     = c('31-12-1992', '31-12-1994', '31-12-1996', '31-12-1998', '31-12-2000', '31-12-2002', '31-12-2004', '31-12-2006', '31-12-2008', '31-12-2010', '31-12-2012', '31-12-2014', '31-12-2016', '31-12-2018', '31-12-2020')
#quary1
query1 <- expand.grid(econ, tax, uncert)
STquery1 = c()
for (j in 1:length(query1$Var1)){
STquery1[j] = paste0(query1[j,1], '%2C+', query1[j,2], '%2C+', query1[j,3])
}
pages <- c()
for (k in 1:2 ){
# economie%20taxe%20incertitudes          01-06-2023                                                    30-06-2023
url1 = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[k],'/?datemin=',start[1],'&datemax=',end[15],'&search_sort=date_asc&page=',1)
webpage  <- read_html(url1)
page <- html_text(html_nodes(webpage,'.river__pagination--page-search'))
page.n <- as.numeric(page)
pages[k] <- max(page.n)
print(k)
}
url1
start
end
url1
pages <- c()
for (k in 1:2){
# economie%20taxe%20incertitudes          01-06-2023                                                    30-06-2023
url1 = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[k],'/?datemin=',start[k],'&datemax=',end[k],'&search_sort=date_asc&page=',1)
webpage  <- read_html(url1)
page <- html_text(html_nodes(webpage,'.river__pagination--page-search'))
page.n <- as.numeric(page)
pages[k] <- max(page.n)
print(k)
}
url1
577 * 20
577 /20
pages <- c()
for (k in 1:length(STquery1)){
# economie%20taxe%20incertitudes          01-06-2023                                                    30-06-2023
url1 = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[k],'/?datemin=',start[k],'&datemax=',end[k],'&search_sort=date_asc&page=',1)
webpage  <- read_html(url1)
page <- html_text(html_nodes(webpage,'.facettes__nombre'))
page.n <- as.numeric(page)
pages[k] <- max(page.n)
print(k)
}
warnings()
View(query1)
pages
page
url1
length(STquery1)
pages <- c()
for (k in 1:length(STquery1)){
url1 = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[k],'/?datemin=',start[1],'&datemax=',end[15],'&search_sort=date_asc&page=',1)
webpage  <- read_html(url1)
page <- html_text(html_nodes(webpage,'.facettes__nombre'))
page.n <- as.numeric(page)
pages[k] <- max(page.n)
print(k)
}
pagers
pagers
pages
page
page.n
?gsub
gsub(".*résultats", "", page)
gsub("résultats", "", page)
gsub(" résultats", "", page)
as.numeric(page)
page1 = gsub(" résultats", "", page)
as.numeric(page1)
pages <- c()
for (k in 1:length(STquery1)){
url1 = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[k],'/?datemin=',start[1],'&datemax=',end[15],'&search_sort=date_asc&page=',1)
webpage  <- read_html(url1)
page <- html_text(html_nodes(webpage,'.facettes__nombre'))
page.n <- as.numeric(gsub(" résultats", "", page))
pages[k] <- max(page.n)
print(k)
}
counts
pages
count <- c()
counts <- c()
for (k in 1:length(STquery1)){
url1 = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[k],'/?datemin=',start[1],'&datemax=',end[15],'&search_sort=date_asc&page=',1)
webpage  <- read_html(url1)
count[k] <- html_text(html_nodes(webpage,'.facettes__nombre'))
count.n <- as.numeric(gsub(" résultats", "", count[k]))
counts[k] <- max(count.n)
print(k)
}
pages
counts
count
#how many pages are for each query
pquery <- cbind(STquery1, counts, count)
View(pquery)
gsub(" résultats", "", count[193])
as.numeric(gsub(" résultats", "", count[193]))
as.numeric(gsub("[^0-9]", "", count[193]))
count <- c()
counts <- c()
for (k in 1:length(STquery1)){
url1[k] = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[k],'/?datemin=',start[1],'&datemax=',end[15],'&search_sort=date_asc&page=',1)
webpage  <- read_html(url1[k])
count[k] <- html_text(html_nodes(webpage,'.facettes__nombre'))
count.n <- as.numeric(gsub("[^0-9]", "", count[k]))
counts[k] <- max(count.n)
print(k)
}
counts
pquery <- cbind(STquery1, url1, counts, count, pages)
View(pquery)
pages <- ceiling(counts/20)
pquery <- cbind(STquery1, url1, counts, count, pages)
rm(econ, tax, uncert, j, k, count.n, webpage, url1, query1)
rm(page)
rm(page.n)
rm(page1)
rm(count)
rm(counts)
k <- c()
for(j in 2:length(pages)){
k[1]=0
k[j] = k[j-1] + pages[j-1]
}
k[1]=pages[1]
k <- c()
for(j in 2:length(pages)){
k[1]=pages[1]
k[j] = k[j-1] + pages[j-1]
}
PUBLIE = list()
for (j in 1:length(STquery1)){    #loop over keywords
for (i in 1:pages[j]){          #loop over pages
url2[j] = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[j],'/?datemin=',start[1],'&datemax=',end[15],'&search_sort=date_asc&page=',i)
# url1 = paste0('https://www.lemonde.fr/recherche/?search_keywords=',STquery1[j],'&start_at=',start[1],'&end_at=',end[15],'&search_sort=date_asc&page=',i)
webpage  <- read_html(url2[j])
PUBLIE[[i + k[j]]] <- html_text(html_nodes(webpage,'.fig-profil-tools , .fig-profil-chapo , a'))
}
print(j)
}
PUBLIE = list()
url2 <- c()
for (j in 1:length(STquery1)){    #loop over keywords
for (i in 1:pages[j]){          #loop over pages
url2[j] = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[j],'/?datemin=',start[1],'&datemax=',end[15],'&search_sort=date_asc&page=',i)
# url1 = paste0('https://www.lemonde.fr/recherche/?search_keywords=',STquery1[j],'&start_at=',start[1],'&end_at=',end[15],'&search_sort=date_asc&page=',i)
webpage  <- read_html(url2[j])
PUBLIE[[i + k[j]]] <- html_text(html_nodes(webpage,'.fig-profil-tools , .fig-profil-chapo , a'))
}
print(j)
}
url2
PUBLIE = list()
url2 <- c()
for (j in 1:2){    #loop over keywords
for (i in 1:pages[j]){          #loop over pages
url2 = paste0('https://recherche.lefigaro.fr/recherche/', STquery1[j],'/?datemin=',start[1],'&datemax=',end[15],'&search_sort=date_asc&page=',i)
# url1 = paste0('https://www.lemonde.fr/recherche/?search_keywords=',STquery1[j],'&start_at=',start[1],'&end_at=',end[15],'&search_sort=date_asc&page=',i)
webpage  <- read_html(url2)
PUBLIE[[i + k[j]]] <- html_text(html_nodes(webpage,'.fig-profil-tools , .fig-profil-chapo , a'))
}
print(j)
}
print('i=' 1)
print ?
d
?print
url2
View(webpage)
View(PUBLIE)
PUBLIE[[34]]
PUBLIE[[29]]
library(pacman)
library(vars) # Load package
install.packages("vars")
library(vars) # Load package
setwd('H:/.shortcut-targets-by-id/1-95-jjFcJ-99DqRccBGFmRS9ZqRftxBC/AnalysisOfEPU/AnalysisOfEPU_Buliskeria/2023VAR')
library(readxl)
data <- read_excel("data.xlsx")
View(data)
var.1 <- VAR(data, 2, type = "none") # Estimate the model
View(data)
library(zoo)
start_date <- as.Date("2001-01-01")
end_date <- as.Date("2019-12-01")
data$time <- as.yearmon(monthly_dates)
monthly_dates <- seq(start_date, end_date, by = "month")
data$time <- as.yearmon(monthly_dates)
View(data)
